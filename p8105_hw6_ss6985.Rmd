---
title: "P8105 Homework 6"
author: "Shehzrin Shah"
date: "2024-11-19"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(123)
```


# Problem 1 

Import the 2017 Central Park weather data. 

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Use 5,000 bootstrap samples, and for each bootstrap sample, produce estimates of r-squared and log(betahat0 * betahat1).

```{r}
bootstrap_rsq = function(lm) {
  r_squared = lm |>
    broom::glance() |>
    pull(r.squared)
  return(r_squared)
}

bootstrap_log = function(lm) {
  log_beta_product = lm |>
    broom::tidy() |>
    pull(estimate) |>
    prod() |>
    log()
  return(log_beta_product)
}

bootstrap_results = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    r_squared = map(models, bootstrap_rsq),
    log_beta_product = map(models, bootstrap_log)
  ) |>
  select(-strap, -models, -.id) |>
  unnest(r_squared) |>
  unnest(log_beta_product)
```

Plot the distribution of these estimates.

```{r}
ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_histogram(binwidth = 0.01, fill = "blue", alpha = 0.7) +
  labs(title = "Bootstrap Distribution of R-squared", x = "R-squared", y = "Frequency")
```

The bootstrap distribution of `r-squared` is unimodal and slightly skewed left. Most values lie between 0.89 and 0.93 (with peak frequency of about 0.91), suggesting a high proportion of the variance in `tmax` is explained by `tmin` in the regression model. 

Plot the distribution of these estimates.

```{r}
ggplot(bootstrap_results, aes(x = log_beta_product)) +
  geom_histogram(binwidth = 0.01, fill = "red", alpha = 0.7) +
  labs(title = "Bootstrap Distribution of Log Beta Product", x = "Log Beta Product", y = "Frequency")
```

The bootstrap distribution of `log_beta_product` is approximately symmetric (roughly normal distribution) and centered around 2.02 (mostly ranging from 1.95 and 2.05). 

Using the 5,000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to produce a 95% confidence interval (CI) for `r-squared` and `log_beta_product`. 
```{r}
bootstrap_results |>
  summarize(
    ci_lower_log_beta_product = quantile(log_beta_product, 0.025),
    ci_upper_log_beta_product = quantile(log_beta_product, 0.975),
    ci_lower_r_squared = quantile(r_squared, 0.025),
    ci_upper_r_squared = quantile(r_squared, 0.975)
  ) |>
  knitr::kable(digits = 4)
```


# Problem 2

Create a `city_state` variable and a binary variable indicating whether the homicide is solved. Omit cities that don't report victim race and Tulsa, AL (data entry mistake). Limit analysis to those for whom `victim_race` is `white` or `black`. Be sure `victim_age` is numeric.

```{r}
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

For Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex, and race as predictors. Obtain the estimate and CI of the adjusted **odds ratio** (OR) for solving homicides comparing male victims to female victims keeping all other variables fixed. 

```{r}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    odds_ratio = exp(estimate), 
    ci_upper_odds_ratio = exp(estimate + 1.96 * std.error),
    ci_lower_odds_ratio = exp(estimate - 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |> 
  select(odds_ratio, ci_lower_odds_ratio, ci_upper_odds_ratio) |>
  knitr::kable(digits = 3)
```

Run `glm` for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Create a dataframe with estimated ORs and CIs for each city.

```{r}
allcities_glm = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = df)),
  tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    odds_ratio = exp(estimate), 
    ci_upper_odds_ratio = exp(estimate + 1.96 * std.error),
    ci_lower_odds_ratio = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, odds_ratio, ci_lower_odds_ratio, ci_upper_odds_ratio)

allcities_glm |>
  knitr::kable(digits = 3)
```

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR.

```{r}
allcities_glm |> 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) |> 
  ggplot(aes(x = city_state, y = odds_ratio)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_lower_odds_ratio, ymax = ci_upper_odds_ratio)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Nearly all cities have odds ratios that are less than or close to 1. This suggests that crimes with male victims hold smaller or similar odds of resolution compared to crimes with female victims, adjusting for victim age and race. Half of the cities have narrow CI that do not contain 1. A few cities have higher ORs with wide CIs (variability or smaller sample sizes). This suggests a significant difference in resolution rates by sex after adjustment for victim age and race. Cities with CIs crossing 1 suggest the effect is not statistically significant.


# Problem 3

Load and clean the data for regression analysis. 

```{r}
birthweight_df = 
  read_csv("birthweight.csv", na = c("", "NA", "Unknown")) |>
  janitor::clean_names() |>
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

sum(is.na(birthweight_df))
```

Propose a regression model for birthweight. 

```{r}
bwt_model = lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + wtgain + malform + ppbmi + babysex + momage + parity, data = birthweight_df)
summary(bwt_model)
```

I selected relevant predictors based on theory including baby attributes (e.g., `bhead`, `blenghth`), maternal health factors (e.g., `delwt`, `smoke`), and pregnancy history (e.g., `parity`, `malform`). I used a linear regression model that it fitted to predict birthweight (`bwt`).  

Plot of model residuals against fitted values.

```{r}
birthweight_df |>
  modelr::add_predictions(bwt_model) |> 
  modelr::add_residuals(bwt_model) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") 
```

Compare original model to one using length at birth and gestational age as predictors and to one using head circumference, length, sex, and all interactions between these. 

```{r}
bwtmodel_2 = lm(bwt ~ gaweeks + blength, data = birthweight_df)
bwtmodel_3 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = birthweight_df)

cv_df = crossv_mc(birthweight_df, n = 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) |>
  mutate(
    mod_mod = map(train, ~lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + wtgain + malform + ppbmi + babysex + momage + parity, data = .x)),
    comp2_mod = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    comp3_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))
  ) |>
  mutate(
    rmse_mod_mod = map2_dbl(mod_mod, test, ~rmse(model = .x, data = .y)),
    rmse_comp2_mod = map2_dbl(comp2_mod, test, ~rmse(model = .x, data = .y)),
    rmse_comp3_mod = map2_dbl(comp3_mod, test, ~rmse(model = .x, data = .y))
  )

cv_df |> 
  summarise(mod1_mean_error = mean(rmse_mod_mod),
            mod2_mean_error = mean(rmse_comp2_mod),
            mod3_mean_error = mean(rmse_comp3_mod)) |>
  knitr::kable(digits = 3)

cv_df |>
  select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse") |>
  mutate(model = fct_inorder(model)) |>
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

Model 1 (original model) shows the lowest RMSE among the 3 models (most accurate predictions for birthweight). Its RMSE distribution is also narrower (most consistent predictions across cross-val folds). Model 2 (gestational age + length) is simpler and has a higher average RMSE compared to Model 1. It explains less variation in birthweight. Its wider RMSE distribution also suggests less reliable performance. Model 3 (head circumference + length + sex + interactions) has a RMSE higher than Model 1 but comparable to Model 2. Even with the interaction terms, it does not do better than Model 1 (maybe because interactions are not adding as much predictive power). The RMSE distribution is slightly wider than Model 1 but narrower than Model 2. 





