P8105 Homework 6
================
Shehzrin Shah
2024-11-19

# Problem 1

Import the 2017 Central Park weather data.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## using cached file: /Users/shehzrin/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2024-09-26 10:19:43.784363 (8.651)

    ## file min/max dates: 1869-01-01 / 2024-09-30

Use 5,000 bootstrap samples, and for each bootstrap sample, produce
estimates of r-squared and log(betahat0 \* betahat1).

``` r
bootstrap_rsq = function(lm) {
  r_squared = lm |>
    broom::glance() |>
    pull(r.squared)
  return(r_squared)
}

bootstrap_log = function(lm) {
  log_beta_product = lm |>
    broom::tidy() |>
    pull(estimate) |>
    prod() |>
    log()
  return(log_beta_product)
}

bootstrap_results = weather_df |>
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    r_squared = map(models, bootstrap_rsq),
    log_beta_product = map(models, bootstrap_log)
  ) |>
  select(-strap, -models, -.id) |>
  unnest(r_squared) |>
  unnest(log_beta_product)
```

Plot the distribution of these estimates.

``` r
ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_histogram(binwidth = 0.01, fill = "blue", alpha = 0.7) +
  labs(title = "Bootstrap Distribution of R-squared", x = "R-squared", y = "Frequency")
```

<img src="p8105_hw6_ss6985_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

The bootstrap distribution of `r-squared` is unimodal \_\_\_\_. Most
values lie between 0.89 and 0.93 (with peak frequency of about 0.91),
suggesting a high proportion of the variance in `tmax` is explained by
`tmin` in the regression model.

Plot the distribution of these estimates.

``` r
ggplot(bootstrap_results, aes(x = log_beta_product)) +
  geom_histogram(binwidth = 0.01, fill = "red", alpha = 0.7) +
  labs(title = "Bootstrap Distribution of Log Beta Product", x = "Log Beta Product", y = "Frequency")
```

<img src="p8105_hw6_ss6985_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

The bootstrap distribution of `log_beta_product` is approximately
symmetric (roughly normal distribution) and centered around 2.02.

Using the 5,000 bootstrap estimates, identify the 2.5% and 97.5%
quantiles to produce a 95% confidence interval (CI) for `r-squared` and
`log_beta_product`.

``` r
bootstrap_results |>
  summarize(
    ci_lower_log_beta_product = quantile(log_beta_product, 0.025),
    ci_upper_log_beta_product = quantile(log_beta_product, 0.975),
    ci_lower_r_squared = quantile(r_squared, 0.025),
    ci_upper_r_squared = quantile(r_squared, 0.975)
  ) |>
  knitr::kable(digits = 4)
```

| ci_lower_log_beta_product | ci_upper_log_beta_product | ci_lower_r_squared | ci_upper_r_squared |
|--------------------------:|--------------------------:|-------------------:|-------------------:|
|                    1.9642 |                    2.0584 |             0.8946 |             0.9271 |

# Problem 2

Create a `city_state` variable and a binary variable indicating whether
the homicide is solved. Omit cities that don’t report victim race and
Tulsa, AL (data entry mistake). Limit analysis to those for whom
`victim_race` is `white` or `black`. Be sure `victim_age` is numeric.

``` r
homicide_df = 
  read_csv("homicide-data.csv", na = c("", "NA", "Unknown")) |>
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

For Baltimore, MD, use the `glm` function to fit a logistic regression
with resolved vs unresolved as the outcome and victim age, sex, and race
as predictors. Obtain the estimate and CI of the adjusted **odds ratio**
for solving homicides comparing male victims to female victims keeping
all other variables fixed.

``` r
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    odds_ratio = exp(estimate), 
    ci_upper_odds_ratio = exp(estimate + 1.96 * std.error),
    ci_lower_odds_ratio = exp(estimate - 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |> 
  select(odds_ratio, ci_lower_odds_ratio, ci_upper_odds_ratio) |>
  knitr::kable(digits = 3)
```

| odds_ratio | ci_lower_odds_ratio | ci_upper_odds_ratio |
|-----------:|--------------------:|--------------------:|
|      0.426 |               0.325 |               0.558 |

Run `glm` for each of the cities in your dataset, and extract the
adjusted odds ratio (and CI) for solving homicides comparing male
victims to female victims. Create a dataframe with estimated ORs and CIs
for each city.

``` r
allcities_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
  tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    odds_ratio = exp(estimate), 
    ci_upper_odds_ratio = exp(estimate + 1.96 * std.error),
    ci_lower_odds_ratio = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, odds_ratio, ci_lower_odds_ratio, ci_upper_odds_ratio)

allcities_results |>
  knitr::kable(digits = 3)
```

| city_state         | odds_ratio | ci_lower_odds_ratio | ci_upper_odds_ratio |
|:-------------------|-----------:|--------------------:|--------------------:|
| Albuquerque, NM    |      1.767 |               0.831 |               3.761 |
| Atlanta, GA        |      1.000 |               0.684 |               1.463 |
| Baltimore, MD      |      0.426 |               0.325 |               0.558 |
| Baton Rouge, LA    |      0.381 |               0.209 |               0.695 |
| Birmingham, AL     |      0.870 |               0.574 |               1.318 |
| Boston, MA         |      0.667 |               0.354 |               1.260 |
| Buffalo, NY        |      0.521 |               0.290 |               0.935 |
| Charlotte, NC      |      0.884 |               0.557 |               1.403 |
| Chicago, IL        |      0.410 |               0.336 |               0.501 |
| Cincinnati, OH     |      0.400 |               0.236 |               0.677 |
| Columbus, OH       |      0.532 |               0.378 |               0.750 |
| Denver, CO         |      0.479 |               0.236 |               0.971 |
| Detroit, MI        |      0.582 |               0.462 |               0.734 |
| Durham, NC         |      0.812 |               0.392 |               1.683 |
| Fort Worth, TX     |      0.669 |               0.397 |               1.127 |
| Fresno, CA         |      1.335 |               0.580 |               3.071 |
| Houston, TX        |      0.711 |               0.558 |               0.907 |
| Indianapolis, IN   |      0.919 |               0.679 |               1.242 |
| Jacksonville, FL   |      0.720 |               0.537 |               0.966 |
| Las Vegas, NV      |      0.837 |               0.608 |               1.154 |
| Long Beach, CA     |      0.410 |               0.156 |               1.082 |
| Los Angeles, CA    |      0.662 |               0.458 |               0.956 |
| Louisville, KY     |      0.491 |               0.305 |               0.790 |
| Memphis, TN        |      0.723 |               0.529 |               0.988 |
| Miami, FL          |      0.515 |               0.304 |               0.872 |
| Milwaukee, wI      |      0.727 |               0.499 |               1.060 |
| Minneapolis, MN    |      0.947 |               0.478 |               1.875 |
| Nashville, TN      |      1.034 |               0.685 |               1.562 |
| New Orleans, LA    |      0.585 |               0.422 |               0.811 |
| New York, NY       |      0.262 |               0.138 |               0.499 |
| Oakland, CA        |      0.563 |               0.365 |               0.868 |
| Oklahoma City, OK  |      0.974 |               0.624 |               1.520 |
| Omaha, NE          |      0.382 |               0.203 |               0.721 |
| Philadelphia, PA   |      0.496 |               0.378 |               0.652 |
| Pittsburgh, PA     |      0.431 |               0.265 |               0.700 |
| Richmond, VA       |      1.006 |               0.498 |               2.033 |
| San Antonio, TX    |      0.705 |               0.398 |               1.249 |
| Sacramento, CA     |      0.669 |               0.335 |               1.337 |
| Savannah, GA       |      0.867 |               0.422 |               1.780 |
| San Bernardino, CA |      0.500 |               0.171 |               1.462 |
| San Diego, CA      |      0.413 |               0.200 |               0.855 |
| San Francisco, CA  |      0.608 |               0.317 |               1.165 |
| St. Louis, MO      |      0.703 |               0.530 |               0.932 |
| Stockton, CA       |      1.352 |               0.621 |               2.942 |
| Tampa, FL          |      0.808 |               0.348 |               1.876 |
| Tulsa, OK          |      0.976 |               0.614 |               1.552 |
| Washington, DC     |      0.691 |               0.469 |               1.018 |

Create a plot that shows the estimated ORs and CIs for each city.
Organize cities according to estimated OR.

``` r
allcities_results |> 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) |> 
  ggplot(aes(x = city_state, y = odds_ratio)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_lower_odds_ratio, ymax = ci_upper_odds_ratio)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

<img src="p8105_hw6_ss6985_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

Nearly all cities have odds ratios that are less than 1. This suggests
that crimes with male victims hold smaller odds of resolution compared
to crimes with female victims, adjusting for victim age and race. Half
of the cities have narrow CI that do not contain 1. This suggests a
signficant difference in resolution rates by sex after adjustment for
victim age and race.

# Problem 3

Load and clean the data for regression analysis.

``` r
birthweight_df = 
  read_csv("birthweight.csv", na = c("", "NA", "Unknown")) |>
  janitor::clean_names() |>
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
sum(is.na(birthweight_df))
```

    ## [1] 0

A regression model for birthweight:. Plot of model residuals against
fitted values.

``` r
bwt_model = lm(bwt ~ gaweeks + ppwt + momage + smoken + wtgain + ppbmi, data = birthweight_df)
summary(bwt_model)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ gaweeks + ppwt + momage + smoken + wtgain + 
    ##     ppbmi, data = birthweight_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1809.28  -270.89     6.71   284.30  1495.10 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -39.5781    97.1220  -0.408    0.684    
    ## gaweeks      58.6467     2.1614  27.133  < 2e-16 ***
    ## ppwt          7.9213     0.6415  12.349  < 2e-16 ***
    ## momage       12.5184     1.7618   7.105 1.40e-12 ***
    ## smoken       -7.7785     0.9056  -8.589  < 2e-16 ***
    ## wtgain        9.8417     0.6246  15.757  < 2e-16 ***
    ## ppbmi       -26.6864     4.0516  -6.587 5.04e-11 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 439.8 on 4335 degrees of freedom
    ## Multiple R-squared:  0.2637, Adjusted R-squared:  0.2627 
    ## F-statistic: 258.8 on 6 and 4335 DF,  p-value: < 2.2e-16

Describe modeling process:

Plot of model residuals against fitted values.

``` r
birthweight_df |>
  modelr::add_predictions(bwt_model) |> 
  modelr::add_residuals(bwt_model) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") 
```

<img src="p8105_hw6_ss6985_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

Comparing original model to 2 others:
